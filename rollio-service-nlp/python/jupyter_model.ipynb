{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports Train\n",
    "from tweet_data_label import train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports Dependencies\n",
    "import spacy\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Validation on Tweet Data\n",
    "def tweet_data_validate(data):\n",
    "    print(f'Length: {len(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 129\n"
     ]
    }
   ],
   "source": [
    "tweet_data_validate(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up spACY library\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize Data Methods\n",
    "def tweet_clean(text):\n",
    "    lower = []\n",
    "    text = text.replace('&amp;', 'and')\n",
    "\n",
    "    for token in nlp(text):\n",
    "        lower.append(token.text.lower())\n",
    "            \n",
    "    return lower\n",
    "\n",
    "# Only target affirmation data\n",
    "def organize_tweet_data(train_data):\n",
    "    affirmation_map = {\n",
    "        'POSITIVE': 0,\n",
    "        'NEGATIVE': 1,\n",
    "        'NULL': 2\n",
    "    }\n",
    "    tweet_sequences = []\n",
    "    target_affirm = []\n",
    "\n",
    "    # Organize tweets & train data into arrays\n",
    "    for tweet_data in train_data:\n",
    "        tweet_sequences.append(tweet_clean(tweet_data[0]))\n",
    "        affirmation = tweet_data[1]['affirmation'][0][0] # Only first affirmation\n",
    "        target_affirm.append(affirmation_map[affirmation])\n",
    "        \n",
    "    return [tweet_sequences, target_affirm]\n",
    "\n",
    "#  Tokenization Methods\n",
    "def tokenize_tweets(tokenizer, data):\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    \n",
    "    return tokenizer.texts_to_sequences(data)\n",
    "    \n",
    "def create_zeros_array(length):\n",
    "    zeros_arr = []\n",
    "\n",
    "    i = 0\n",
    "    while i < length:\n",
    "        zeros_arr.append(0)\n",
    "        i += 1\n",
    "\n",
    "    return zeros_arr\n",
    "\n",
    "def find_max_tweet_len(data):\n",
    "    max_len = 0\n",
    "\n",
    "    # Find longest sequence, pad everything to its length\n",
    "    for tweet in data:\n",
    "        seq_len = len(tweet)\n",
    "        if seq_len > max_len:\n",
    "            max_len = seq_len\n",
    "\n",
    "    return max_len\n",
    "\n",
    "def pad_array(data, max_len):\n",
    "    zeros_len = max_len - len(data)\n",
    "    zeros_arr = create_zeros_array(zeros_len)\n",
    "\n",
    "    return [*data,*zeros_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize & Pad Tweet Data\n",
    "tweet_sequences, target_affirm = organize_tweet_data(train_data)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenized_data = tokenize_tweets(tokenizer, tweet_sequences)\n",
    "\n",
    "max_len = find_max_tweet_len(tokenized_data)\n",
    "\n",
    "X = [pad_array(data, max_len) for data in tokenized_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 785\n",
      "Target Affirm Length: 129\n"
     ]
    }
   ],
   "source": [
    "# Check size of tokenized vocab\n",
    "vocab_size = len(tokenizer.index_word)\n",
    "\n",
    "print(f'Vocab Size: {vocab_size}')\n",
    "print(f'Target Affirm Length: {len(target_affirm)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokenized data to Numpy Array\n",
    "X = np.array(X)\n",
    "y = to_categorical(target_affirm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (129, 34)\n",
      "y shape: (129, 3)\n"
     ]
    }
   ],
   "source": [
    "# Validate Data\n",
    "print(f'X shape: {X.shape}')\n",
    "print(f'y shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train\n",
      "X_train len: 86\n",
      "\n",
      "y_train\n",
      "y_train len: 86\n"
     ]
    }
   ],
   "source": [
    "# Validate Data\n",
    "print('X_train')\n",
    "print(f'X_train len: {len(X_train)}')\n",
    "# print(X_train)\n",
    "print('')\n",
    "print('y_train')\n",
    "print(f'y_train len: {len(y_train)}')\n",
    "# print(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Train Data\n",
    "def scale_data():\n",
    "    scaler_object = MinMaxScaler()\n",
    "    scaler_object.fit(X_train)\n",
    "    \n",
    "    scaled_X_train = scaler_object.transform(X_train)\n",
    "    scaled_X_test = scaler_object.transform(X_test)\n",
    "    return [scaled_X_train, scaled_X_test]\n",
    "\n",
    "scaled_X_train, scaled_X_test = scale_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Training Model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8,input_dim=34,activation='relu'))\n",
    "    model.add(Dense(8,input_dim=34,activation='relu'))\n",
    "    model.add(Dense(3,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 8)                 280       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 379\n",
      "Trainable params: 379\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "# Review Model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-e1a6aee6466d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Setup Class Weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# print(y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclass_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# print(class_weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# class_weight = {\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_course/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_course/lib/python3.7/site-packages/sklearn/utils/class_weight.py\u001b[0m in \u001b[0;36mcompute_class_weight\u001b[0;34m(class_weight, classes, y)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         raise ValueError(\"classes should include all valid labels that can \"\n\u001b[1;32m     45\u001b[0m                          \"be in y\")\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "# Setup Class Weights\n",
    "# print(y)\n",
    "# class_weights = class_weight.compute_class_weight('balanced', np.unique(y), y)\n",
    "# print(class_weights)\n",
    "# class_weight = {\n",
    "#     0: 1.,\n",
    "#     1: 50.,\n",
    "#     2: 2.\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "model.fit(scaled_X_train,y_train,epochs=10,verbose=2, shuffle=True, class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "X : [2 2 2 2 1 2 0 2 0 2 0 2 2 2 2 2 2 0 2 0 0 2 2 2 2 2 0 2 2 2 0 2 2 0 2 0 2\n",
      " 0 2 2 2 0 2] 43\n",
      "Y : [2 2 0 0 2 0 0 0 0 0 2 0 2 2 2 1 2 0 0 0 0 0 2 0 2 2 0 2 2 2 0 2 0 0 2 0 1\n",
      " 0 2 0 2 0 2]\n",
      "\n",
      "Confusion Matrix\n",
      "[[11  0 11]\n",
      " [ 0  0  2]\n",
      " [ 1  1 17]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.50      0.65        22\n",
      "           1       0.00      0.00      0.00         2\n",
      "           2       0.57      0.89      0.69        19\n",
      "\n",
      "    accuracy                           0.65        43\n",
      "   macro avg       0.49      0.46      0.45        43\n",
      "weighted avg       0.72      0.65      0.64        43\n",
      "\n",
      "Accuracy Score\n",
      "0.6511627906976745\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "model.predict_classes(scaled_X_test)\n",
    "predictions = model.predict_classes(scaled_X_test)\n",
    "\n",
    "print('Results')\n",
    "print(f'X : {predictions} {len(predictions)}')\n",
    "print(f'Y : {y_test.argmax(axis=1)}\\n')\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(f'{confusion_matrix(y_test.argmax(axis=1),predictions)}\\n')\n",
    "\n",
    "print('Classification Report')\n",
    "print(classification_report(y_test.argmax(axis=1),predictions))\n",
    "\n",
    "print('Accuracy Score')\n",
    "print(accuracy_score(y_test.argmax(axis=1),predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
